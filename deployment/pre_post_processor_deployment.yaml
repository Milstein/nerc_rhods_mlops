---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
    app.kubernetes.io/part-of: intelligent-application
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ia-inference
      app.kubernetes.io/component: ia-inference
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: ia-inference
        app.kubernetes.io/component: ia-inference
    spec:
      containers:
        - name: ia-inference
          image: >-
            quay.io/rh-aiservices-bu/mad-m6-workshop-inference-service:latest
          env:
            - name: GRPC_HOST
              value: 'modelmesh-serving'
            - name: GRPC_PORT
              value: '8033'
            - name: MODEL_NAME
              value: 'coolstore'
            - name: CONF_THRESHOLD
              value: '0.4'
            - name: IOU_THRESHOLD
              value: '0.6'
            - name: COUPON_VALUE
              value: '[5,10,15]'
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
---
kind: Service
apiVersion: v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
    app.kubernetes.io/part-of: intelligent-application
spec:
  ports:
    - name: 8000-tcp
      protocol: TCP
      port: 8000
      targetPort: 8000
  selector:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: ia-inference
  labels:
    app: ia-inference
    app.kubernetes.io/component: ia-inference
    app.kubernetes.io/instance: ia-inference
    app.kubernetes.io/name: ia-inference
    app.kubernetes.io/part-of: intelligent-application
spec:
  to:
    kind: Service
    name: ia-inference
    weight: 100
  port:
    targetPort: 8000-tcp
  tls:
    termination: edge
  wildcardPolicy: None
